# Configuration for Pong RL Training

experiment:
  name: "pong_smolvlm_rl"
  description: "LLM-based RL for Atari Pong with trial-reflect-finetune loop"

# Model configuration
model:
  name: "HuggingFaceTB/SmolVLM-Instruct"
  dtype: "bfloat16"  # Use bfloat16 for A100
  device: "cuda"

# Environment configuration
environment:
  game: "Pong"
  max_steps_per_episode: 1000
  frame_size: [384, 384]  # SmolVLM input size

# Training loop configuration
training_loop:
  num_iterations: 5  # Number of trial-reflect-finetune cycles
  episodes_per_iteration: 100  # Episodes to collect per iteration

# Trial phase configuration
trial:
  temperature: 0.7  # Sampling temperature for agent
  max_new_tokens: 100  # Max tokens for action generation
  save_frequency: 10  # Save episodes every N episodes

# Reflection phase configuration
reflection:
  reward_threshold: 0.0  # Only reflect on rewards <= this value
  sample_rate: 0.3  # Fraction of eligible steps to reflect on (to save compute)
  reflection_temperature: 0.7
  max_reflection_tokens: 300

# Fine-tuning configuration
finetuning:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  warmup_steps: 100
  max_grad_norm: 1.0
  save_steps: 100
  logging_steps: 10
  bf16: true  # Use bfloat16 training

# Resource configuration
resources:
  gpu_memory_gb: 40  # A100 40GB
  num_workers: 4  # Data loader workers

# Output configuration
output:
  base_dir: "data/experiments"
  save_checkpoints: true
  save_episodes: true
  save_reflections: true
  log_to_tensorboard: true
  log_to_wandb: false  # Set to true if using W&B

# Evaluation configuration
evaluation:
  eval_frequency: 1  # Evaluate every N iterations
  eval_episodes: 10  # Number of episodes for evaluation
